# coding=utf-8
# @Time : 2024/8/15 21:53
# @File : bert_lstm_class.py
# @Software : PyCharm
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split


# 剔除标点符号,\xa0 空格


class bert_lstm(nn.Module):
    def __init__(self, bertpath, hidden_dim, output_size, n_layers, use_cuda, bidirectional=True, drop_prob=0.5):
        super(bert_lstm, self).__init__()
        self.use_cuda = use_cuda
        self.output_size = output_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.bidirectional = bidirectional

        # Bert ----------------重点，bert模型需要嵌入到自定义模型里面
        self.bert = BertModel.from_pretrained(bertpath)
        for param in self.bert.parameters():
            param.requires_grad = True

        # LSTM layers
        self.lstm = nn.LSTM(768, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional)

        # dropout layer
        self.dropout = nn.Dropout(drop_prob)

        # linear and sigmoid layers
        if bidirectional:
            self.fc = nn.Linear(hidden_dim * 2, output_size)
        else:
            self.fc = nn.Linear(hidden_dim, output_size)

        # self.sig = nn.Sigmoid()

    def forward(self, x, hidden, attention_mask, token_type_ids):
        batch_size = x.size(0)
        # 生成bert字向量
        x = self.bert(x, attention_mask, token_type_ids)[0]  # bert 字向量

        # lstm_out
        # x = x.float()
        lstm_out, (hidden_last, cn_last) = self.lstm(x, hidden)
        # print(lstm_out.shape)   #[32,100,768]
        # print(hidden_last.shape)   #[4, 32, 384]
        # print(cn_last.shape)    #[4, 32, 384]

        # 修改 双向的需要单独处理
        if self.bidirectional:
            # 正向最后一层，最后一个时刻
            hidden_last_L = hidden_last[-2]
            # print(hidden_last_L.shape)  #[32, 384]
            # 反向最后一层，最后一个时刻
            hidden_last_R = hidden_last[-1]
            # print(hidden_last_R.shape)   #[32, 384]
            # 进行拼接
            hidden_last_out = torch.cat([hidden_last_L, hidden_last_R], dim=-1)
            # print(hidden_last_out.shape,'hidden_last_out')   #[32, 768]
        else:
            hidden_last_out = hidden_last[-1]  # [32, 384]

        # dropout and fully-connected layer
        out = self.dropout(hidden_last_out)
        # print(out.shape)    #[32,768]
        out = self.fc(out)

        return out

    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data

        number = 1
        if self.bidirectional:
            number = 2

        if self.use_cuda:
            hidden = (weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().float().cuda(),
                      weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().float().cuda()
                      )
        else:
            hidden = (weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().float(),
                      weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().float()
                      )

        return hidden


def pretreatment(comments):
    result_comments = []
    punctuation = '。，？！：%&~（）、；“”&|,.?!:%&~();""[]'
    for comment in comments:
        comment = ''.join([c for c in comment if c not in punctuation])
        comment = ''.join(comment.split())  # \xa0
        result_comments.append(comment)
    return result_comments


def train_model(config, data_train):
    net = bert_lstm(
        config.bert_path,
        config.hidden_dim,
        config.output_size,
        config.n_layers,
        config.use_cuda,
        config.bidirectional,
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=config.lr)
    if config.use_cuda:
        net.cuda()
    net.train()
    for e in range(config.epochs):
        # initialize hidden state
        h = net.init_hidden(config.batch_size)
        counter = 0
        # batch loop
        for inputs, attention_mask, token_type_ids, labels in data_train:
            counter += 1
            if config.use_cuda:
                inputs, attention_mask, token_type_ids, labels = inputs.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()
            h = tuple([each.data for each in h])
            net.zero_grad()
            output = net(inputs, h, attention_mask, token_type_ids)
            loss = criterion(output.squeeze(), labels.long())
            loss.backward()
            optimizer.step()
            if counter % config.print_every == 0:
                out = output.argmax(dim=1)
                accuracy = (out == labels).sum().item() / len(labels)
                print(f'Epoch {e + 1}/{config.epochs}, Step {counter}, Loss: {loss.data}, Accuary:{accuracy}')
    torch.save(net.state_dict(), config.save_path)


def test_model(config, data_test):
    net = bert_lstm(config.bert_path,
                    config.hidden_dim,
                    config.output_size,
                    config.n_layers,
                    config.use_cuda,
                    config.bidirectional)
    net.load_state_dict(torch.load(config.save_path))
    net.cuda()
    criterion = nn.CrossEntropyLoss()
    test_losses = []  # track loss
    num_correct = 0

    # init hidden state
    h = net.init_hidden(config.batch_size)

    net.eval()
    # iterate over test data
    label_ls = []
    pred_ls = []
    for inputs, attention_mask, token_type_ids, labels in data_test:
        h = tuple([each.data for each in h])
        if config.USE_CUDA:
            inputs, attention_mask, token_type_ids, labels = inputs.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()
        output = net(inputs, h, attention_mask, token_type_ids)
        test_loss = criterion(output.squeeze(), labels.long())
        test_losses.append(test_loss.item())

        output = torch.nn.Softmax(dim=1)(output)
        pred = torch.max(output, 1)[1]

        pred_label_ls = []
        # for single_output in output:
        #     print(f'single_output: {single_output}')
        for pred_label in pred:
            pred_label_ls.append(pred_label.item())
        true_label_ls = []
        for label in labels:
            true_label_ls.append(label.item())
        # print(f'pred_label_ls: {pred_label_ls}')
        # compare predictions to true label
        correct_tensor = pred.eq(labels.long().view_as(pred))
        correct = np.squeeze(correct_tensor.numpy()) if not config.USE_CUDA else np.squeeze(
            correct_tensor.cpu().numpy())
        num_correct += np.sum(correct)
        label_ls += true_label_ls
        pred_ls += pred_label_ls
    print("Test loss: {:.3f}".format(np.mean(test_losses)))
    return get_acc_p_r_f1(label_ls, pred_ls)


def get_acc_p_r_f1(trues, preds, verbose=True):
    TP, FP, FN, TN = 0, 0, 0, 0
    for i in range(len(trues)):
        if trues[i] == 1:
            if preds[i] == 1:
                TP += 1
            elif preds[i] == 0:
                FP += 1
            else:
                raise NotImplementedError
        if trues[i] == 0:
            if preds[i] == 1:
                TN += 1
            elif preds[i] == 0:
                FN += 1
            else:
                raise NotImplementedError
    try:
        accuracy = (TP + TN) / (TP + TN + FP + FN)
    except ZeroDivisionError:
        accuracy = 0
    try:
        precision = TP / (TP + FP)
    except ZeroDivisionError:
        precision = 0
    try:
        recall = TP / (TP + FN)
    except ZeroDivisionError:
        recall = 0
    try:
        f1 = 2 * precision * recall / (precision + recall)
    except ZeroDivisionError:
        f1 = 0
    if verbose:
        print('TP, FP, FN, TN')
        print(TP, FP, FN, TN)
        print('precision, recall, f1, accuracy')
        print([precision, recall, f1, accuracy])
    return [precision, recall, f1, accuracy]
